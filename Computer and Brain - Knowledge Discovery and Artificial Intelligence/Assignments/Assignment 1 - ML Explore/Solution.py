# -*- coding: utf-8 -*-
"""ML_explore (Ashutosh Jha - 11011).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/177o-i-X-y6f8IKMfNqaQ1VZLalBATen2
"""

# just coz I always just do this.
# I am sure this will be used not matter what!
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# check import sklearn
# feel free to import whatever you need
import sklearn

"""## Task - ML Explore

### Part 1 - Loading Datasets
[**Datasets**](https://scikit-learn.org/stable/datasets/index.html)
* Boston
* Iris
* Diabetes
* Digits
* Wine

### Part 2 - Data exploration
* Pandas description
* Matplotlib visualisation
* PCA

### Part 3 - Data Preparation
* [Preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing)
    * StandardScalar
    * MinMaxScalar
    * Normalization
    * Discretization
* Train test split

### Part 4 - Setup models
* [Pipeline](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)
* Supervised
    * logistic/linear regression
    * Support vector machine
    * Naive bayes
    * Decision trees
    * KNN
    * Ensemble methods
* Clustering (unsupervised)
    * k means
    * Affinity propagation

### Part 5 - Training

### Part 6 - Prediction

### Part 7 - Evaluation

**This according to me is one of the most important parts**
* [Classification Metrics](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics)
    * Accuracy Score
    * F1_score
    * Confusion matrix
    * Precision recall curve
* [Regression Metrics](https://scikit-learn.org/stable/modules/classes.html#regression-metrics)
    * Mean absolute error
    * Mean squared error
    * R2 score

* [Clustering Metrics](https://scikit-learn.org/stable/modules/classes.html#clustering-metrics)
    * Adjusted rand score
    * V measure score
    * Contingency matrix

### Part 8 - Model Tuning
* Manual evaluation
* Grid search
* Randomized search
* Cross validation (Also an evaluation method)
* Validation curves

**SOLUTION**
"""

from sklearn import datasets

dataset = datasets.load_wine(return_X_y = False)

x = dataset.data
y = dataset.target

"""**Splitting into training**

"""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.4, random_state = 0)

"""**Feature scaling**"""

from sklearn.preprocessing import StandardScaler
std = StandardScaler()
x_train = std.fit_transform(x_train)
x_test = std.transform(x_test)

"""**Training the model**"""

from sklearn.linear_model import LogisticRegression
lr_classifer = LogisticRegression(random_state = 0)
lr_classifer.fit(x_train, y_train)

lr_y_pred = lr_classifer.predict(x_test)

"""
**Confusion Matrix and Accuracy Score**"""

from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import plot_precision_recall_curve
import matplotlib.pyplot as plt
print(confusion_matrix(y_test, lr_y_pred))
print(accuracy_score(y_test, lr_y_pred))
print(f1_score(y_test, lr_y_pred, average='macro'))

#The dataset is not binary therefore we use average
print(precision_score(y_test, lr_y_pred, average = 'micro'))
print(recall_score(y_test, lr_y_pred, average = 'micro'))

"""**K-Fold Cross Validation**"""

from sklearn.model_selection import cross_val_score
accuracy = cross_val_score(estimator = lr_classifer, X = x_train, y = y_train, cv = 20)
print(accuracy)
print(accuracy.mean())
print(accuracy.std())

"""**Decision Trees**"""

from sklearn.tree import DecisionTreeClassifier
dc = DecisionTreeClassifier(random_state=0)
dc.fit(x_train, y_train)

new_pred = dc.predict(x_test)

from sklearn.metrics import confusion_matrix, accuracy_score, f1_score
print(confusion_matrix(y_test, new_pred))
print(accuracy_score(y_test, new_pred))
print(f1_score(y_test, new_pred, average='macro'))

#The dataset is not binary therefore we use average
print(precision_score(y_test, new_pred, average = 'micro'))
print(recall_score(y_test, new_pred, average = 'micro'))

"""## Experiment !
| No. | Name | Accuracy(in %) | F1_score | Confusion Matrix | Precision_score | Recall_curve | 
| --- | --- | --- | --- | --- | --- | --- |
| 1 | Logistic Regression | 97.85 | 1 | [[14  0  0],[ 0 16  0],[ 0  0  6]] | 1 | 1 |
| 2 | Decision Tress | 97.22 | 0.9777 | [[14  0  0], [ 1 15  0], [ 0  0  6]] | 0.972 | 0.972 |


## Experimenting with Spliting the data
| No. | test_size | Accuracy(in %) | F1_score | Confusion Matrix | Precision_score | Recall_curve | 
| --- | --- | --- | --- | --- | --- | --- |
| 1 | 0.2 | 97.85 | 1 | [[14  0  0] , [ 0 16  0] , [ 0  0  6]] | 1 | 1 |
| 2 | 0.22 | 97.857 | 1 | [[14  0  0], [ 0 18  0] , [ 0  0  8]] | 1 | 1 |
| 3 | 0.3 | 95.47 | 1 | [[19  0  0] , [ 0 22  0] , [ 0  0 13]] | 1 | 1 |
| 4 | 0.4 | 97.1666 | 0.9859885105786744 |[[22  0  0] , [ 0 30  1] , [ 0  0 19]]] | 0.9861111111111112 | 0.9861111111111112 |

"""